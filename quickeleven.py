#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
xdog_quick_demo.py
å¿«é€Ÿæ ·ä¾‹ç‰ˆï¼šåªæ‰«æœ€æ–° N ä¸ªåŒºå—ï¼Œåˆ†é’Ÿçº§å‡ºç»“æœã€‚
è¾“å‡ºæ–‡ä»¶ä¸åŸå…¨é‡ç‰ˆå®Œå…¨å…¼å®¹ï¼š
  xdog_edges.csv
  dust_report.json      â†’ å®æ—¶è¿½åŠ ç‰ˆï¼ˆjsonlinesï¼‰
  real_holding_top.csv
  dust_graph.gexf       â†’ å®æ—¶è¿½åŠ è¾¹ï¼ˆcsvï¼‰
æ–°å¢ï¼šè½»é‡åŒ–æ–­ç‚¹ç»­æ‰«ï¼ˆå•æ•°å­—å¤´ï¼‰ï¼Œæ”¯æŒ --reset / --rewind / --fullï¼ˆç¬¬ä¸€æ¬¡æ‰«å…¨é“¾ï¼‰
      âœ… å®æ—¶å†™è¾¹ã€å¤´ã€ä½™é¢ã€å€™é€‰ã€ç²‰å°˜å †ã€å›¾è¾¹â€”â€”å…¨éƒ¨é€å—è¿½åŠ ï¼ŒCtrl-C åªä¸¢æœ€åä¸€æ¡
"""
import os, json, time, functools, argparse, csv
from typing import Dict, List, Set
import pandas as pd
import networkx as nx
from tqdm import tqdm
from web3 import Web3
from eth_utils import to_checksum_address
from collections import defaultdict

# ---------- ç”¨æˆ·é…ç½® ----------
TOKEN_CONTRACT = Web3.to_checksum_address("")
RPC_URL        = "https://rpc.xlayer.tech"
SAMPLE_BLOCKS  = 67000_00                   # None=ç¬¬ä¸€æ¬¡æ‰«å…¨é“¾ï¼›å¯æ”¹ 2000 ç­‰
MAX_BLOCKS_ONCE= 100
BALANCE_THRESHOLD_WEI = 1000000 * 10**18   # 100 ä¸‡æš
MAX_OUT_TX     = 2
CLUSTER_MIN_ADDR = 5
ENRICH_MIN     = 0.05
PRICE_USD      = 0.20
# å®æ—¶æ–‡ä»¶
CAND_FILE      = "dust_candidate.jsonl"    # æ¯åœ°å€ä¸€è¡Œ
REPORT_FILE    = "dust_report.jsonl"       # æ¯å †ä¸€è¡Œ
GRAPH_EDGE_FILE= "dust_graph.edges.csv"    # å®æ—¶è¾¹
# ------------------------------

w3 = Web3(Web3.HTTPProvider(RPC_URL, request_kwargs={"timeout": 60}))
if not w3.is_connected():
    raise RuntimeError("RPC æ— æ³•è¿æ¥")

TRANSFER_TOPIC = "0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef"
CACHE_FILE     = "cache_bal_nonce.json"
SCAN_HEAD_FILE = "scan_head.json"      # æ–­ç‚¹æ–‡ä»¶ï¼šåªå­˜ {"head": 12345}

# ====================== é€šç”¨å·¥å…· ======================
def retry(func):
    @functools.wraps(func)
    def wrapper(*a, **kw):
        for i in range(5):
            try:
                return func(*a, **kw)
            except Exception as e:
                print(f"[retry {i+1}/5] {e}")
                time.sleep(2**i)
        raise
    return wrapper

@retry
def fetch_events(from_block: int, to_block: int):
    return w3.eth.get_logs({
        'fromBlock': from_block,
        'toBlock': to_block,
        'address': TOKEN_CONTRACT,
        'topics': [TRANSFER_TOPIC]
    })

def parse_event(evt):
    fr = to_checksum_address('0x' + evt['topics'][1].hex()[-40:])
    to = to_checksum_address('0x' + evt['topics'][2].hex()[-40:])
    data = evt['data']
    if isinstance(data, bytes):
        val = int.from_bytes(data, byteorder='big')
    else:
        val = int(data, 16)
    return fr, to, val

# ====================== è½»é‡åŒ–æ–­ç‚¹ç»­æ‰«æ¡†æ¶ ======================
def _load_scan_head() -> int:
    if os.path.exists(SCAN_HEAD_FILE):
        return json.load(open(SCAN_HEAD_FILE)).get("head")
    return None

def _save_scan_head(head: int):
    with open(SCAN_HEAD_FILE, "w", encoding="utf-8") as f:
        json.dump({"head": head}, f, indent=2)
        os.fsync(f.fileno())          # å¼ºåˆ¶åˆ·ç›˜

def get_scan_range(latest: int, sample_blocks: int = SAMPLE_BLOCKS, rewind: int = 0, full: bool = False):
    if sample_blocks is None:
        sample_blocks = latest + 1 if full else SAMPLE_BLOCKS
    head = _load_scan_head()
    if head is None or rewind:
        if sample_blocks is None or full:
            sample_blocks = latest + 1
        head = (latest - sample_blocks) if head is None else head - rewind
        head = max(head, 0)
    start = head + 1
    end   = min(start + sample_blocks - 1, latest)
    if start > end:
        return None, None
    return start, end

def update_scan_head(end: int):
    _save_scan_head(end)

# ====================== æ‰«é“¾ï¼ˆå®æ—¶å†™è¾¹+å®æ—¶å†™å¤´ï¼‰ ======================
# ====================== æ‰«é“¾ï¼ˆå®æ—¶å†™è¾¹+å¤´ï¼‰ ======================
def scan_sample_events():
    latest = w3.eth.block_number
    start, end = get_scan_range(latest, sample_blocks=SAMPLE_BLOCKS, full=args.full)
    if start is None:
        print("å·²è¿½ä¸Šæœ€æ–°å—ï¼Œæ— æ–°åŒºé—´å¯æ‰«")
        return pd.DataFrame(columns=['from', 'to', 'value'])   # â† ç©ºè¡¨ä¹Ÿå¸¦åˆ—å

    print(f"æ–­ç‚¹ç»­æ‰«ï¼š{start} ~ {end}  ï¼ˆæœ€æ–° {latest}ï¼‰")
    csv_path = "xdog_edges.csv"
    write_header = not os.path.exists(csv_path)
    with open(csv_path, "a", newline="", encoding="utf-8") as f:
        writer = csv.writer(f)
        if write_header:
            writer.writerow(["from", "to", "value"])

        for _from in tqdm(range(start, end + 1, MAX_BLOCKS_ONCE),
                          total=(end - start) // MAX_BLOCKS_ONCE + 1,
                          desc="Quick Scan"):
            _to = min(_from + MAX_BLOCKS_ONCE - 1, end)
            evts = fetch_events(_from, _to)
            for e in evts:
                fr, to, val = parse_event(e)
                if fr == to:
                    continue
                writer.writerow([fr, to, val])
            update_scan_head(_to)

    # 2. ç©ºæ–‡ä»¶ä¿æŠ¤ + åˆ—åç»Ÿä¸€
    if os.path.getsize(csv_path):
        df = pd.read_csv(csv_path)
    else:
        df = pd.DataFrame(columns=['from', 'to', 'value'])  # 0 è¡Œä¹Ÿå¸¦åˆ—å
    df.columns = df.columns.str.strip().str.lower().str.replace('"', '')  # å»ç©ºæ ¼/å¼•å·/å°å†™
    return df

# ====================== ä½™é¢ & nonceï¼ˆå®æ—¶å†™å€™é€‰ï¼‰ ======================
def load_cache() -> Dict[str, Dict[str, int]]:
    if os.path.exists(CACHE_FILE):
        return json.load(open(CACHE_FILE, encoding="utf-8"))
    return {}

def save_cache(cache: Dict[str, Dict[str, int]]):
    with open(CACHE_FILE, "w", encoding="utf-8") as f:
        json.dump(cache, f, indent=2)

@retry
def _get_balance(a, contract, block):
    return contract.functions.balanceOf(a).call(block_identifier=block)

@retry
def _get_nonce(a, block):
    return w3.eth.get_transaction_count(a, block_identifier=block)

def batch_balance_and_nonce(all_addrs: Set[str]):
    abi = [{"constant": True, "inputs": [{"name": "_owner", "type": "address"}],
            "name": "balanceOf", "outputs": [{"name": "balance", "type": "uint256"}], "type": "function"}]
    contract = w3.eth.contract(address=TOKEN_CONTRACT, abi=abi)
    latest_safe = w3.eth.block_number
    cache = load_cache()
    todo = list(all_addrs - set(cache.keys()))
    print(f"ç¼“å­˜å‘½ä¸­ {len(cache)} æ¡ï¼Œå‰©ä½™ {len(todo)} æ¡å¾…æŸ¥è¯¢")

    # å®æ—¶å€™é€‰æ–‡ä»¶
    from jsonlines import open as jopen
    with jopen(CAND_FILE, 'a') as cf:
        for a in tqdm(todo, desc="Balance&Nonce"):
            try:
                bal = _get_balance(a, contract, latest_safe)
                nonce = _get_nonce(a, latest_safe)
                cache[a] = {"bal": bal, "nonce": nonce}
                save_cache(cache)
                # ğŸ”¥å®æ—¶å†™å€™é€‰
                if bal <= BALANCE_THRESHOLD_WEI and nonce <= MAX_OUT_TX + 1:
                    cf.write({"addr": a, "bal": bal, "nonce": nonce})
            except Exception as e:
                print(f"\n{a} æŸ¥è¯¢å¤±è´¥: {e}ï¼Œå·²è·³è¿‡")
                continue

    # ç»Ÿä¸€è¿”å›æ ¼å¼
    bal_map, nonce_map = {}, {}
    for a in all_addrs:
        bal_map[a]   = cache[a]["bal"]
        nonce_map[a] = cache[a]["nonce"]
    return bal_map, nonce_map

# ====================== ç²‰å°˜å †æ£€æµ‹ï¼ˆå®æ—¶è¿½åŠ ï¼‰ ======================
def detect_dust_clusters(df_edges, bal_map, nonce_map):
        # åˆ—åä¿®å¤ & ç©ºè¡¨ä¿æŠ¤
    if df_edges.empty:
        print("df_edges ä¸ºç©ºï¼Œè·³è¿‡èšç±»")
        return {}
    df_edges.columns = df_edges.columns.str.strip().str.lower()  # ç»Ÿä¸€å°å†™
    for col in ['from', 'to', 'value']:
        if col not in df_edges.columns:
            print(f"ç¼ºåˆ— {col}ï¼Œè·³è¿‡èšç±»")
            return {}
    # åªè¯» **æ–°å€™é€‰åœ°å€**
    from jsonlines import open as jopen
    new_cand = [ln["addr"] for ln in jopen(CAND_FILE)] if os.path.exists(CAND_FILE) else []
    if not new_cand:
        return {}

    G = nx.DiGraph()
    # ç®€åŒ–ä¸ºå…¨é‡è¯»è¾¹ï¼ˆå¯ä¼˜åŒ–ä¸ºåªè¯»æ–°è¾¹ï¼‰
    for _, r in df_edges.iterrows():
        G.add_edge(r['from'], r['to'], value=int(r['value']))

    parent = defaultdict(set)
    for _, r in df_edges.iterrows():
        src, dst = r['from'], r['to']
        if dst in new_cand:
            parent[dst].add(src)

    cluster = defaultdict(list)
    for addr in new_cand:
        if len(parent[addr]) == 1:
            upper = list(parent[addr])[0]
            cluster[upper].append(addr)

    total_addr = len(new_cand)
    total_bal  = sum(bal_map[a] for a in new_cand)

    # å®æ—¶è¿½åŠ æ¨¡å¼å†™æŠ¥å‘Š & å›¾è¾¹
    with jopen(REPORT_FILE, 'a') as rf, open(GRAPH_EDGE_FILE, 'a', newline='') as ge:
        writer = csv.writer(ge)
        if not os.path.exists(GRAPH_EDGE_FILE):
            writer.writerow(["upper", "child"])          # è¡¨å¤´åªå†™ä¸€æ¬¡
        for upper, children in cluster.items():
            c_bal = sum(bal_map[a] for a in children)
            enrich = (len(children) / max(total_addr, 1)) * (c_bal / max(total_bal, 1))
            if len(children) >= CLUSTER_MIN_ADDR and enrich >= ENRICH_MIN:
                rf.write({
                    "upper": upper,
                    "children_cnt": len(children),
                    "children_balance": str(c_bal),
                    "enrich_ratio": enrich,
                    "children": children
                })
                for ch in children:
                    writer.writerow([upper, ch])         # å®æ—¶å†™å›¾è¾¹

    # è¿”å›å…¨é‡ reportï¼ˆä¾›åç»­æ’è¡Œç”¨ï¼‰
    report = {}
    if os.path.exists(REPORT_FILE):
        for ln in jopen(REPORT_FILE):
            report[ln["upper"]] = {
                "children_cnt": ln["children_cnt"],
                "children_balance": ln["children_balance"],
                "enrich_ratio": ln["enrich_ratio"],
                "children": ln["children"]
            }
    return report

# ====================== çœŸå®æŒä»“æ’è¡Œ ======================
def real_holding_rank(df_edges, bal_map, dust_report):
    real = []
    for up, meta in dust_report.items():
        children = meta["children"]
        total_bal = bal_map.get(up, 0) + sum(bal_map.get(a, 0) for a in children)
        outflow = 0
        for _, r in df_edges.iterrows():
            if r['from'] == up and r['to'] not in children:
                outflow += int(r['value'])
        real.append((up, total_bal, outflow, len(children)))
    handled = set(dust_report.keys())
    dust_addrs = {a for meta in dust_report.values() for a in meta["children"]}
    for a in bal_map:
        if a in handled or a in dust_addrs:
            continue
        real.append((a, bal_map[a], 0, 0))
    real.sort(key=lambda x: x[1], reverse=True)
    df = pd.DataFrame(real, columns=['address', 'real_balance_wei', 'outflow_wei', 'dust_count'])
    df['real_balance'] = df['real_balance_wei'] / 10 ** 18
    df['outflow'] = df['outflow_wei'] / 10 ** 18
    df['rank'] = df['real_balance'].rank(method='min', ascending=False).astype(int)
    df['value_usd'] = df['real_balance'] * PRICE_USD
    df[['rank', 'address', 'real_balance', 'value_usd', 'outflow', 'dust_count']].to_csv(
        "real_holding_top.csv", index=False, float_format='%.6f')
    print("\n====== çœŸå®æŒä»“ TOP-20 ======")
    print(df.head(20)[['rank', 'address', 'real_balance', 'value_usd', 'dust_count']])
    return df

# ====================== main ======================
def main():
    global args
    ap = argparse.ArgumentParser()
    ap.add_argument("--reset", action="store_true", help="åˆ é™¤æ‰€æœ‰æ–­ç‚¹ä¸ csvï¼Œä»å¤´å…¨é‡")
    ap.add_argument("--rewind", type=int, default=0, help="å¼ºåˆ¶å‘å‰å›é€€ N å—å†å¼€å§‹æ‰«")
    ap.add_argument("--full", action="store_true", help="ç¬¬ä¸€æ¬¡æ‰«å…¨é“¾ï¼ˆç­‰ä»· SAMPLE_BLOCKS=Noneï¼‰")
    args = ap.parse_args()

    if args.reset:
        for f in [SCAN_HEAD_FILE, "xdog_edges.csv", CAND_FILE, REPORT_FILE, GRAPH_EDGE_FILE]:
            os.remove(f) if os.path.exists(f) else None
        print("å·²æ¸…é™¤æ‰€æœ‰å®æ—¶æ–‡ä»¶ï¼Œå³å°†ä»å¤´å…¨é‡")

#    edges = scan_sample_events()
    # 2. å†™åœ¨ main() é‡Œ
    edges = scan_sample_events()
    if edges.empty or 'from' not in edges.columns or 'to' not in edges.columns:
        print("âš ï¸  xdog_edges.csv ä¸ºç©ºæˆ–ç¼ºåˆ—ï¼Œç¨‹åºç»ˆæ­¢ã€‚")
        return
    all_addrs = set(edges['from']) | set(edges['to'])
    all_addrs = set(edges['from']) | set(edges['to'])
    bal_map, nonce_map = batch_balance_and_nonce(all_addrs)
    report = detect_dust_clusters(edges, bal_map, nonce_map)
    print(f"æ£€å‡ºç²‰å°˜å † {len(report)} ç»„")
    real_holding_rank(edges, bal_map, report)
    print("\nQuickDemo å®Œæˆï¼è¾“å‡ºæ–‡ä»¶ï¼š\n"
          "  xdog_edges.csv        â€“ æ ·æœ¬è½¬è´¦è¾¹ï¼ˆå®æ—¶è¿½åŠ ï¼‰\n"
          "  dust_candidate.jsonl  â€“ å€™é€‰åœ°å€ï¼ˆå®æ—¶è¿½åŠ ï¼‰\n"
          "  dust_report.jsonl     â€“ ç²‰å°˜å †ï¼ˆå®æ—¶è¿½åŠ ï¼‰\n"
          "  dust_graph.edges.csv  â€“ å›¾è¾¹ï¼ˆå®æ—¶è¿½åŠ ï¼‰\n"
          "  real_holding_top.csv  â€“ çœŸå®æŒä»“")

if __name__ == "__main__":
    main()